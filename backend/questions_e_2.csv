Question:問題文,Options[0]:正解選択肢,Options[1]:不正解選択肢1,Options[2]:不正解選択肢2,Options[3]:不正解選択肢3,Supplement:捕捉情報,Difficulty:難易度
"行列の積ABが定義されるための条件はどれか？","Aの列数とBの行数が等しい","AとBが正方行列である","AとBが同じ次元である","AとBが逆行列を持つ","行列の積は、Aの列数とBの行数が等しい場合に定義されます。",1
"テンソルの積において、要素ごとの積を指すものはどれか？","アダマール積","クロネッカー積","テンソル積","外積","アダマール積は同じ次元の行列やテンソルの要素ごとの積です。",1
"行列のランクを求める際に使用されるのはどれか？","階段行列","単位行列","零行列","対角行列","階段行列に変形することで行列のランクを調べます。",1
"固有値分解が可能な行列の条件はどれか？","正方行列","対称行列","直交行列","対角行列","固有値分解は正方行列に対して定義されます。",1
"特異値分解で得られる特異値は何を表すか？","行列の尺度","行列のランク","行列の逆行列","行列の固有値","特異値は行列の尺度やエネルギーを表します。",1
"確率変数XとYが独立である場合、次の関係はどれか？","P(X,Y)=P(X)P(Y)","P(X|Y)=P(X)","P(Y|X)=P(Y)","P(X,Y)=P(X)+P(Y)","独立な確率変数では、同時確率は各確率の積になります。",1
"条件付き確率の定義として正しいものはどれか？","P(A|B)=P(A∩B)/P(B)","P(A|B)=P(B)/P(A)","P(A|B)=P(A)+P(B)","P(A|B)=P(A∩B)×P(B)","条件付き確率はP(A|B)=P(A∩B)/P(B)で定義されます。",1
"ベルヌーイ分布の期待値はどれか？","成功確率p","失敗確率1-p","0","1","ベルヌーイ分布の期待値は成功確率pです。",1
"中心極限定理が示すのはどれか？","大数の法則","確率変数の和が正規分布に近づく","サンプリングエラーの減少","分散の不偏推定","独立同分布の確率変数の和は正規分布に近づくことを示します。",1
"最尤推定法で最大化するものはどれか？","尤度関数","費用関数","損失関数","エントロピー","最尤推定法では尤度関数を最大化します。",1
"ナイーブベイズ分類器が持つ仮定はどれか？","特徴量間の独立性","データの線形分離可能性","データの均一分布","特徴量間の相関性","ナイーブベイズは特徴量間が独立であると仮定します。",1
"自己情報量を表す式はどれか？","-log₂P(x)","P(x)log₂P(x)","-P(x)log₂P(x)","log₂(1-P(x))","自己情報量はI(x) = -log₂P(x)で計算されます。",1
"エントロピーが最大になるのはどのような分布か？","一様分布","正規分布","ベルヌーイ分布","ポアソン分布","エントロピーは一様分布のときに最大になります。",1
"K近傍法で分類を行う際に必要なものはどれか？","距離の計算","確率分布の仮定","勾配の計算","目的関数の最小化","K近傍法ではデータ間の距離を計算します。",1
"ユークリッド距離を計算する際に使用するのはどれか？","2乗和の平方根","絶対値の和","最大差の値","コサイン類似度","ユークリッド距離は2乗和の平方根で計算します。",1
"次のうち、ニューラルネットワークの活性化関数として最も一般的に使用されるものはどれか？","ReLU","シグモイド関数","ソフトマックス関数","tanh関数","ReLUは最も一般的に使用される活性化関数です。",2
"次のうち、正則化によって過学習を防ぐために使用される手法はどれか？","ドロップアウト","交差エントロピー","データの標準化","データの正規化","ドロップアウトは正則化の一種であり、過学習を防ぎます。",1
"次のうち、線形回帰モデルにおける誤差の評価指標として適切なものはどれか？","平均二乗誤差","クロスエントロピー","相互情報量","精度","平均二乗誤差は線形回帰モデルの誤差を評価する指標です。",1
"次のうち、決定木のノード分割に使用される基準はどれか？","Gini係数","相互情報量","クロスエントロピー","相関係数","Gini係数は決定木のノード分割に使用される基準です。",1
"ニューラルネットワークの活性化関数として最も一般的に使用されるものはどれか？","ReLU","シグモイド関数","ソフトマックス関数","tanh関数","ReLUは最も一般的に使用される活性化関数です。",2
"線形回帰モデルにおける誤差の評価指標として適切なものはどれか？","平均二乗誤差","クロスエントロピー","相互情報量","精度","平均二乗誤差は線形回帰モデルの誤差を評価する指標です。",1
"教師あり学習の例として正しいものはどれか？","回帰分析","クラスタリング","次元削減","主成分分析","回帰分析は教師あり学習の一種です。",1
"Lasso回帰で使用される正則化はどれか？","L1正則化","L2正則化","エラスティックネット","ドロップアウト","Lasso回帰ではL1正則化を用います。",1
"ロジスティック回帰で用いられる活性化関数はどれか？","シグモイド関数","ReLU関数","ソフトマックス関数","双曲線正接関数","ロジスティック回帰ではシグモイド関数を使用します。",1
"サポートベクターマシンでマージンを最大化する理由はどれか？","汎化性能を高めるため","計算コストを減らすため","過学習を防ぐため","クラス間の相互関係を強めるため","マージンを最大化することで汎化性能が向上します。",1
"ROC曲線のAUC値が表すのはどれか？","分類器の性能指標","データの分散","モデルの複雑さ","学習の速度","AUCはROC曲線下の面積で、分類性能を示します。",1
"多層パーセプトロンにおいて、各層の出力に適用される関数はどれか？","活性化関数","損失関数","コスト関数","確率密度関数","活性化関数は各層の出力に適用されます。",1
"深層学習で使用される損失関数で、2値分類に適したものはどれか？","バイナリクロスエントロピー","平均二乗誤差","ソフトマックスクロスエントロピー","ヒンジ損失","2値分類にはバイナリクロスエントロピーが適しています。",1
"ReLU関数の特性として正しいものはどれか？","負の値を0にする","出力が常に1になる","入力値をそのまま出力する","値を0と1の間に圧縮する","ReLU関数は負の値を0にします。",1
"勾配消失問題を軽減するために有効な活性化関数はどれか？","ReLU関数","シグモイド関数","tanh関数","ソフトマックス関数","ReLU関数は勾配消失問題を軽減します。",1
"次のうち、LSTMの一部として使われるゲートの一つはどれか？","忘却ゲート","入力ゲート","出力ゲート","重み更新ゲート","忘却ゲートはLSTMの一部として使用されます。",3
"次のうち、ガウス分布の特性として正しいものはどれか？","平均と分散で完全に記述される","中央値とモードで記述される","多峰性分布である","カテゴリカル分布である","ガウス分布は平均と分散で完全に記述されます。",1
"L1正則化の効果はどれか？","パラメータのスパース化","過学習の促進","計算速度の向上","データの正規化","L1正則化はパラメータをスパース化します。",1
"ドロップアウトの目的はどれか？","過学習の防止","計算コストの削減","データの前処理","モデルの評価","ドロップアウトは過学習を防ぐために用いられます。",1
"畳み込みニューラルネットワークで、画像の空間的な特徴を捉える層はどれか？","畳み込み層","全結合層","プーリング層","正規化層","畳み込み層は空間的な特徴を抽出します。",1
"次のうち、機械学習における過学習を防ぐための技術はどれか？","正則化","勾配消失","シグモイド関数","最急降下法","正則化は過学習を防ぐための技術です。",1
"次のうち、教師あり学習の手法はどれか？","ロジスティック回帰","主成分分析","t-SNE","k-meansクラスタリング","ロジスティック回帰は教師あり学習の手法です。",1
"プーリング層の主な役割はどれか？","特徴マップのダウンサンプリング","パラメータ数の増加","勾配の計算","データの正規化","プーリング層は特徴マップをダウンサンプリングします。",1
"RNNにおける勾配消失問題を解決するために開発されたものはどれか？","LSTM","CNN","GAN","autoencoder","LSTMは勾配消失問題を解決するためのRNNです。",1
"LSTMの構成要素でないものはどれか？","プーリングゲート","忘却ゲート","入力ゲート","出力ゲート","プーリングゲートはLSTMの構成要素ではありません。",1
"Transformerにおいて自己注意機構がもたらす効果はどれか？","長距離依存関係の効果的な学習","モデルの軽量化","データ拡張の自動化","勾配消失の防止","自己注意機構は長距離依存関係の学習を可能にします。",1
"ハイパーパラメータの最適化手法で、探索空間を事前分布に基づいて効率的に検索する方法はどれか？","ベイズ最適化","グリッドサーチ","ランダムサーチ","遺伝的アルゴリズム","ベイズ最適化は事前分布を用いて効率的に探索します。",1
"過学習を防ぐためにデータを増やす技術はどれか？","データ拡張","ドロップアウト","バッチ正規化","早期終了","データ拡張はデータセットを増やして過学習を防ぎます。",1
"画像データの拡張手法でないものはどれか？","スペックアグメント","フリップ","回転","クロップ","スペックアグメントは音声データの拡張手法です。",1
"Dockerは何を提供するためのツールか？","コンテナ型仮想化","ハイパーバイザー","データベース管理","ネットワーク監視","Dockerはコンテナ型仮想化を提供します。",1
"GPUによる計算の高速化で利用される原理はどれか？","SIMD（単一命令複数データ）","MIMD（複数命令複数データ）","SISD（単一命令単一データ）","MISD（複数命令単一データ）","GPUはSIMDアーキテクチャを利用します。",1
"次のうち、データを複数のクラスタに分ける手法はどれか？","k-meansクラスタリング","主成分分析","線形回帰","ロジスティック回帰","k-meansクラスタリングはデータを複数のクラスタに分ける手法です。",1
"次のうち、ディープラーニングモデルのトレーニングにおいて、学習率を適応的に調整するアルゴリズムはどれか？","Adam","SGD","RMSprop","最急降下法","Adamは学習率を適応的に調整するアルゴリズムです。",3
"モデルの軽量化手法で、不要なパラメータを削減するものはどれか？","プルーニング（枝刈り）","量子化","蒸留","圧縮","プルーニングは不要なパラメータを削減します。",1
"教師なし学習の例として正しいものはどれか？","クラスタリング","回帰分析","分類","ロジスティック回帰","クラスタリングは教師なし学習の一種です。",1
"t-SNEの主な利用目的はどれか？","高次元データの可視化","モデルの評価","データの正規化","パラメータの最適化","t-SNEは高次元データを低次元に可視化します。",1
"Autoencoderのボトルネック層の役割はどれか？","データの圧縮","活性化関数の適用","勾配の計算","データの拡張","ボトルネック層はデータを低次元に圧縮します。",1
"GANにおいて、生成器と識別器が競い合う関係を何と呼ぶか？","敵対的関係","協調関係","従属関係","並列関係","GANは生成器と識別器の敵対的関係を利用します。",1
"DQNが解決を目指す問題はどれか？","強化学習における連続値の行動選択","教師なし学習のクラスタリング","自然言語処理の文生成","画像分類の精度向上","DQNは強化学習で連続値の行動選択を扱います。",1
"転移学習で、既存モデルの知識を新しいタスクに適用する際の方法はどれか？","ファインチューニング","ゼロからの学習","モデルの破棄","データの再収集","ファインチューニングは既存モデルを微調整します。",1
"モデルの説明性を高めるために、入力特徴量の重要度を計算する手法はどれか？","SHAP値","t-SNE","PCA","LDA","SHAP値は特徴量の重要度を定量化します。",1
"連合学習（Federated Learning）の主な利点はどれか？","データのプライバシーを保護しつつ学習可能","計算資源の集約","データの一元管理","モデルのサイズ削減","連合学習はプライバシーを保護しながら学習できます。",1
"GPUが得意とする処理はどれか？","並列計算","論理演算","単一処理","データベース管理","GPUは並列計算を高速に行えます。",1
"モデルの量子化が目的とすることはどれか？","モデルの軽量化と高速化","勾配の安定化","学習率の自動調整","パラメータの増加","量子化はモデルを軽量化し、推論を高速化します。",1
"教師あり学習と教師なし学習の組み合わせで行われる学習はどれか？","半教師あり学習","強化学習","深層学習","転移学習","半教師あり学習は両者を組み合わせた学習です。",1
"XAI（Explainable AI）が重要視される理由はどれか？","AIの判断根拠を理解・説明するため","計算速度を上げるため","データ保存量を減らすため","モデルの複雑さを増すため","XAIはAIの判断根拠を明らかにします。",1
"ノイズ除去オートエンコーダ（Denoising Autoencoder）の目的はどれか？","入力データからノイズを除去する","データの拡張","モデルの軽量化","勾配消失の防止","ノイズ除去オートエンコーダはノイズを除去します。",1
"一様分布の確率密度関数において、特定の範囲外の値の確率はどれか？","0","1","範囲に依存する","無限大","一様分布では範囲外の確率は0です。",1
"次元の呪いが発生する主な原因はどれか？","データの高次元化によるデータ疎密度の増加","データ量の不足","モデルの過学習","計算資源の不足","高次元になるとデータが疎になり解析が困難になります。",1
"注意（Attention）機構が解決を目指す問題はどれか？","長い系列データの依存関係の学習","データの不足","計算コストの増加","モデルの過学習","Attentionは長い系列での依存関係を学習します。",1
"VAE（Variational AutoEncoder）の特徴はどれか？","生成モデルとして機能する","分類性能を高める","計算速度を上げる","データをクラスタリングする","VAEはデータの生成モデルとして機能します。",1
"ハミルトニアンモンテカルロ法が関連するのはどの分野か？","ベイズ推定","クラスタリング","強化学習","画像認識","ハミルトニアンモンテカルロ法はベイズ推定で使われます。",1
"スパース表現を促進する正則化手法はどれか？","L1正則化","L2正則化","ドロップアウト","バッチ正規化","L1正則化はスパースなモデルを促進します。",1
"決定木の深さを制限することの主な効果はどれか？","過学習の防止","計算時間の増加","モデルの複雑化","データの増加","深さを制限すると過学習を防げます。",1
"非線形な特性を捉えるためにカーネル法を用いるアルゴリズムはどれか？","サポートベクターマシン","ロジスティック回帰","線形回帰","ナイーブベイズ","カーネル法はSVMで用いられます。",1
"データの分布を仮定せずに距離情報のみでクラスタリングを行う手法はどれか？","DBSCAN","k-means","主成分分析","線形判別分析","DBSCANはデータ分布を仮定せずにクラスタリングします。",1
