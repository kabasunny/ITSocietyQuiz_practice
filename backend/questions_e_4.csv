Question:問題文,Options[0]:正解選択肢,Options[1]:不正解選択肢1,Options[2]:不正解選択肢2,Options[3]:不正解選択肢3,Supplement:補足情報,Difficulty:難易度
"ResNetが導入した深層学習モデルの技術はどれか？","残差接続（skip-connection）","注意機構（Attention Mechanism）","バッチ正規化（Batch Normalization）","ドロップアウト（Dropout）","ResNetは残差接続を導入し、深いネットワークの学習を可能にしました。",1
"Vision Transformerで位置情報を保持するために使用されるのはどれか？","位置エンコーディング（Position Embedding）","セグメントエンコーディング（Segment Embedding）","CLSトークン","マスキング（Masking）","位置エンコーディングはトークンの位置情報を保持するために使用されます。",1
"Faster R-CNNにおいて、物体の候補領域を提案するネットワークはどれか？","Region Proposal Network (RPN)","Selective Search","ROIプーリング","アンカー（Anchor）","RPNは物体候補領域を高速に提案します。",1
"YOLOが分類される物体検出モデルのタイプはどれか？","1ステージ検出","2ステージ検出","アンカーベース検出","アンカーフリー検出","YOLOは1ステージで物体検出を行います。",1
"セマンティックセグメンテーションでピクセル単位の分類を行うために使用されるモデルはどれか？","Fully Convolutional Network (FCN)","サポートベクトルマシン","リカレントニューラルネットワーク","オートエンコーダ","FCNはセマンティックセグメンテーションで使用されます。",1
"Word2Vecの手法の一つで、コンテキストからターゲット単語を予測するモデルはどれか？","Continuous Bag of Words (CBOW)","Skip-Gram","ネガティブサンプリング","n-gram","CBOWはコンテキストからターゲット単語を予測します。",1
"BERTの事前学習タスクで、文章の連続性を予測するものはどれか？","Next Sentence Prediction (NSP)","Masked Language Modeling (MLM)","Language Modeling (LM)","Text Classification","NSPは次の文章が連続するかを予測します。",1
"GPTモデルが得意とする学習方法はどれか？","Few Shot Learning","教師なし学習","強化学習","半教師あり学習","GPTモデルはFew Shot Learningを得意とします。",1
"音声信号を周波数領域に変換するための手法はどれか？","短時間フーリエ変換","サンプリング","メル尺度","ケプストラム","短時間フーリエ変換は時間軸上の信号を周波数領域に変換します。",1
"WaveNetで用いられる畳み込みの手法はどれか？","ダイレイテッドカジュアル畳み込み","深層畳み込み","転置畳み込み","ポイントワイズ畳み込み","WaveNetはダイレイテッドカジュアル畳み込みを使用します。",1
"オートエンコーダの中でもデータのノイズ耐性を高めるために設計されたものはどれか？","デノイジングオートエンコーダ","変分オートエンコーダ（VAE）","スパースオートエンコーダ","スタックドオートエンコーダ","デノイジングオートエンコーダは入力データからノイズを除去します。",1
"GANのトレーニング中に発生する、生成データの多様性が失われる問題はどれか？","モード崩壊（mode collapse）","勾配消失","過学習","過剰適合","モード崩壊は生成データの多様性が失われる問題です。",1
"DQNにおける経験を再利用するための手法はどれか？","Experience Replay","ターゲットネットワーク","Qテーブル","価値関数近似","Experience Replayは過去の経験を再利用します。",1
"深層強化学習において、アクターとクリティックを用いる手法はどれか？","Actor-Critic法","Q学習","SARSA","モンテカルロ法","Actor-Critic法はアクターとクリティックで学習します。",1
"転移学習で、新しいタスクに適応するために行われる手法はどれか？","ファインチューニング","特徴抽出のみを利用する","モデルの再構築","データの再収集","ファインチューニングは既存モデルを微調整します。",1
"半教師あり学習で、ラベルなしデータを活用するための手法はどれか？","Self-Training","強化学習","クラスタリング","エントロピー最小化","Self-Trainingはラベルなしデータを活用します。",1
"Active Learningで、教師データとして選択すべきデータを決定する指標はどれか？","Uncertainty Sampling","ランダムサンプリング","確率サンプリング","バイアスサンプリング","Uncertainty Samplingは不確実性の高いデータを選びます。",1
"2つの入力サンプル間の類似度を学習するネットワークはどれか？","Siamese Network","オートエンコーダ","リカレントニューラルネットワーク","畳み込みニューラルネットワーク","Siamese Networkは2つの入力間の類似度を学習します。",1
"Triplet Lossにおいて、アンカー、ポジティブ、もう一つのサンプルは何と呼ばれるか？","ネガティブサンプル","サポートサンプル","ニュートラルサンプル","コントラストサンプル","Triplet Lossはアンカー、ポジティブ、ネガティブを使います。",1
"メタ学習で、様々なタスクに迅速に適応するために学習するものはどれか？","モデルの初期値","特定のタスクの重み","ハイパーパラメータ","データの前処理方法","メタ学習ではモデルの初期値を学習します。",1
"深層学習モデルの判断根拠を視覚的に示す手法はどれか？","Grad-CAM","LIME","シャープレー値","t-SNE","Grad-CAMは判断根拠をヒートマップで示します。",1
"モデルの予測結果に対する局所的な解釈を与える手法はどれか？","LIME","SHAP","CAM","PCA","LIMEは局所的なモデル解釈を提供します。",1
"モデルの軽量化手法で、学習済みモデルの知識を小さいモデルに移すものはどれか？","知識蒸留（Distillation）","プルーニング（枝刈り）","量子化（Quantization）","ハフ変換","知識蒸留は大きなモデルの知識を小さなモデルに移します。",1
"分散深層学習で、複数のデバイスで同じモデルを学習する手法はどれか？","データ並列化","モデル並列化","ハイブリッド並列化","パイプライン並列化","データ並列化は同じモデルを各デバイスで異なるデータを使って学習します。",1
"連合学習において、各デバイスのモデルを統合するための手法はどれか？","Federated Averaging","モデル蒸留","アンサンブル学習","勾配降下法","Federated Averagingは各デバイスのモデルパラメータを平均化します。",1
"GPUが採用する並列処理アーキテクチャはどれか？","SIMT（単一命令複数スレッド）","SIMD（単一命令複数データ）","MIMD（複数命令複数データ）","MISD（複数命令単一データ）","GPUはSIMTアーキテクチャを採用しています。",1
"コンテナ型仮想化で、コンテナの動作を定義するファイルはどれか？","Dockerfile","Docker Hub","コンテナイメージ","仮想マシン","Dockerfileはコンテナの環境構築を定義します。",1
"ResNetの構造で、ネットワークの深さが深くなると発生する問題を解決する技術はどれか？","残差接続（skip-connection）","ドロップアウト","バッチ正規化","アテンション機構","残差接続は勾配消失問題を緩和します。",1
"Vision Transformerが画像を処理する際に、画像を分割する単位はどれか？","パッチ（Patch）","ピクセル","フィルタ","特徴マップ","Vision Transformerは画像をパッチに分割します。",1
"Mask R-CNNがFaster R-CNNに追加した機能はどれか？","インスタンスセグメンテーション","物体分類","ランドマーク検出","領域提案","Mask R-CNNはインスタンスセグメンテーションを追加しました。",1
"YOLOが従来の物体検出手法よりも高速である理由はどれか？","1ステージで検出と分類を同時に行う","高精度な領域提案を行う","大規模なアンサンブルを使用する","GPU最適化がされている","YOLOは1ステージで処理を完了するため高速です。",1
"FCNにおいて、元の画像サイズと同じサイズの出力を得るために使用される手法はどれか？","アップサンプリング","ダウンサンプリング","プーリング","畳み込み","アップサンプリングは出力を拡大します。",1
"Word2Vecで、ターゲット単語から周辺単語を予測するモデルはどれか？","Skip-Gram","CBOW","ネガティブサンプリング","n-gram","Skip-Gramはターゲットから周辺単語を予測します。",1
"BERTが事前学習で使用するタスクで、単語をマスクして予測するものはどれか？","Masked Language Modeling (MLM)","Next Sentence Prediction (NSP)","言語モデル","テキスト生成","MLMはマスクした単語を予測します。",1
"GPTモデルが主に使用するアーキテクチャはどれか？","Transformerのデコーダ部分","Transformerのエンコーダ部分","リカレントニューラルネットワーク","畳み込みニューラルネットワーク","GPTはTransformerのデコーダを使用します。",1
"音声信号のサンプリングで、元の信号を完全に復元するために必要な最小サンプリング周波数はどれか？","ナイキスト周波数","シャノン周波数","ローパス周波数","フォーリエ周波数","ナイキスト周波数は信号の最高周波数の2倍です。",1
"ケプストラムが主に利用される音声処理タスクはどれか？","音声認識","ノイズ除去","音声合成","音声検出","ケプストラムは音声認識で重要な特徴抽出です。",1
"CTC損失が適用される主なタスクはどれか？","音声認識のエンドツーエンドモデル","画像分類","機械翻訳","テキスト生成","CTC損失は音声認識で使用されます。",1
"拡散モデルが得意とするタスクはどれか？","画像や音声の生成","分類問題","回帰分析","クラスタリング","拡散モデルは生成タスクで使用されます。",1
"VAEにおいて、連続的な潜在変数のサンプリングを可能にする技術はどれか？","リパラメータizationトリック","勾配消失の回避","Batch Normalization","ドロップアウト","リパラメータizationトリックで勾配計算が可能になります。",1
"CycleGANが解決する問題はどれか？","ペアデータなしでの画像間の変換","音声からテキストへの変換","テキスト生成","時系列予測","CycleGANは未対応のペアデータ間で変換します。",1
"DQNの強化学習で、ターゲットネットワークを使用する理由はどれか？","学習の安定化","計算コストの削減","モデルの軽量化","報酬関数の最適化","ターゲットネットワークは学習の安定化に寄与します。",1
"A3Cが使用する学習の特徴はどれか？","非同期的に複数のエージェントを学習","同期的に一つのエージェントを学習","モデルの蒸留を行う","教師あり学習を適用する","A3Cは非同期に複数のエージェントを学習します。",1
"Contrastive Learningで、データポイント間の類似度を高めるために使用される損失はどれか？","コントラスト損失","クロスエントロピー損失","平均二乗誤差","ヒンジ損失","コントラスト損失は類似度を学習します。",1
"SHAP値が基づいている理論はどれか？","協力ゲーム理論","確率論","線形代数","情報理論","SHAP値は協力ゲーム理論に基づきます。",1
"モデルの量子化で主に変換されるのはどれか？","パラメータのビット幅","モデルの構造","データの前処理","ハイパーパラメータの数","量子化はパラメータを低ビット幅に変換します。",1
"連合学習で、データを中央サーバーに送信しない理由はどれか？","プライバシーの保護","通信コストの削減","計算速度の向上","モデルの精度向上","連合学習は各デバイスのデータをローカルに保持します。",1
"デバイス並列化と比較して、モデル並列化の利点はどれか？","メモリ消費の分散","学習速度の向上","通信オーバーヘッドの削減","モデルの精度向上","モデル並列化は大きなモデルを複数のデバイスに分散します。",1
"Dockerで、コンテナのベースとなるイメージを取得する場所はどれか？","Docker Hub","GitHub","ローカルレジストリ","AWS S3","Docker Hubは公式のコンテナイメージを提供します。",1
"Transformerにおいて、トークン間の関係性を計算する機構はどれか？","自己注意機構（Self-Attention）","畳み込み","リカレント接続","プーリング","Self-Attentionはトークン間の依存関係を捉えます。",1
"Batch Normalizationの効果として正しくないものはどれか？","モデルの表現力を低下させる","学習を安定化させる","収束を早める","過学習を防ぐ","モデルの表現力は低下しません。",1
"ReLU関数を使用する際に適切な重みの初期化方法はどれか？","Heの初期化（Kaiming初期化）","Xavierの初期化","ランダム初期化","ゼロ初期化","ReLUではHeの初期化が適しています。",1
"非線形次元削減手法であるt-SNEが直面する課題はどれか？","高コストな計算量","データの線形性の仮定","モデルの解釈性の低さ","過学習のしやすさ","t-SNEは計算コストが高いです。",1
"過学習を検出するための指標はどれか？","訓練誤差と検証誤差の乖離","高い精度","低い損失値","高いF値","訓練誤差が低く、検証誤差が高いと過学習の可能性があります。",1
"学習率を徐々に減少させることで得られる効果はどれか？","収束の安定化","計算時間の短縮","パラメータの増加","データ量の削減","学習率の減少は収束の安定化に寄与します。",1
"ハイパーパラメータの調整で、探索空間全体を網羅的に検索する手法はどれか？","グリッドサーチ","ランダムサーチ","ベイズ最適化","進化戦略","グリッドサーチは全ての組み合わせを試します。",1
"画像のデータ拡張で、明るさやコントラストを変更する手法はどれか？","光学的変換","幾何学的変換","スペクトラム変換","ノイズ付加","光学的変換は明るさやコントラストを変更します。",1
"深層学習モデルが入力ノイズに対して頑健でない場合に起こりうる問題はどれか？","敵対的攻撃に対する脆弱性","モデルの軽量化","計算速度の向上","データの一般化","入力ノイズに弱いと敵対的攻撃に脆弱です。",1
"モデル蒸留で、教師モデルと生徒モデルが異なる点はどれか？","モデルのサイズ","データの種類","学習タスク","損失関数","生徒モデルは小さいサイズになります。",1
"クラスタリング手法で、階層的クラスタリングを視覚的に表現する図はどれか？","デンドログラム","ヒストグラム","散布図","箱ひげ図","デンドログラムは階層的クラスタリングの結果を示します。",1
"エッジデバイスでのモデル推論時に重要なモデルの特性はどれか？","軽量かつ低消費電力","高い計算資源の要求","モデルの複雑さ","大量のメモリ消費","エッジデバイスでは軽量であることが重要です。",1
"データ正規化の手法で、各特徴量の平均を0、分散を1にするものはどれか？","標準化（Standardization）","正規化（Normalization）","ワンホットエンコーディング","ラベルエンコーディング","標準化は平均0、分散1にします。",1
"時系列データの予測で、長期的な依存関係を学習できるモデルはどれか？","LSTM","単純リカレントニューラルネットワーク","線形回帰","k近傍法","LSTMは長期的な依存関係を学習できます。",1
"ニューラルネットワークにおいて、勾配消失問題が特に深刻となるのはどのような場合か？","層が非常に深い場合","層が浅い場合","大きな学習率を使用する場合","大量のデータを使用する場合","深いネットワークでは勾配消失が発生しやすいです。",1
